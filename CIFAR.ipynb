{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQurpL1EVAyY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmpEg5WwD8Jw",
        "outputId": "11655b13-65aa-4892-a814-32900f6e97e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch 1/5 - Avg Loss: 2.5542\n",
            "Epoch 2/5 - Avg Loss: 2.3792\n",
            "Epoch 3/5 - Avg Loss: 2.3095\n",
            "Epoch 4/5 - Avg Loss: 2.2676\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import logging\n",
        "import itertools\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Data Augmentation and Normalization\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_size = int(0.9 * len(train_set))\n",
        "val_size = len(train_set) - train_size\n",
        "train_sub, val_sub = random_split(train_set, [train_size, val_size])\n",
        "\n",
        "labels = train_set.classes\n",
        "\n",
        "# Define Feedforward Neural Network\n",
        "class FeedforwardNN(nn.Module):\n",
        "    def __init__(self, input_dim=32*32*3, hidden_layers=[128, 64, 32], num_classes=10, activation='relu', dropout=0.3):\n",
        "        super(FeedforwardNN, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = input_dim\n",
        "        act_fn = nn.ReLU() if activation == 'relu' else nn.Sigmoid()\n",
        "\n",
        "        for h in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_size, h))\n",
        "            layers.append(nn.BatchNorm1d(h))\n",
        "            layers.append(act_fn)\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev_size = h\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, num_classes))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.network(x)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.network:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "# Function to Get Optimizer\n",
        "def get_optimizer(model, opt_type='adam', lr=1e-3, weight_decay=0):\n",
        "    optimizers = {\n",
        "        'sgd': optim.SGD(model.parameters(), lr=lr),\n",
        "        'momentum': optim.SGD(model.parameters(), lr=lr, momentum=0.9),\n",
        "        'nesterov': optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True),\n",
        "        'rmsprop': optim.RMSprop(model.parameters(), lr=lr),\n",
        "        'adam': optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay),\n",
        "        'nadam': optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "    }\n",
        "    return optimizers[opt_type]\n",
        "\n",
        "# Training Function\n",
        "def train(model, train_loader, val_loader, optimizer, loss_fn, epochs=10):\n",
        "    model.to(DEVICE)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, lbls in train_loader:\n",
        "            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(imgs)\n",
        "            loss = loss_fn(preds, lbls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    return evaluate(model, val_loader)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, loader):\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in loader:\n",
        "            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
        "            preds = model(imgs)\n",
        "            _, pred_classes = torch.max(preds, 1)\n",
        "            total += lbls.size(0)\n",
        "            correct += (pred_classes == lbls).sum().item()\n",
        "    acc = round(100 * correct / total, 2)\n",
        "    return acc\n",
        "\n",
        "# Confusion Matrix Plot\n",
        "def plot_cm(model, loader):\n",
        "    model.eval()\n",
        "    preds_list, lbls_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in loader:\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            preds = model(imgs)\n",
        "            _, pred_classes = torch.max(preds, 1)\n",
        "            preds_list.extend(pred_classes.cpu().numpy())\n",
        "            lbls_list.extend(lbls.numpy())\n",
        "    cm = confusion_matrix(lbls_list, preds_list)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Run Experiments\n",
        "hyperparams = itertools.product(\n",
        "    [5, 10],  # epochs\n",
        "    [[128, 64, 32], [256, 128, 64]],  # hidden layers\n",
        "    ['sgd', 'adam'],  # optimizers\n",
        "    [1e-3, 1e-4],  # learning rates\n",
        "    [0, 0.0005]  # weight decay\n",
        ")\n",
        "\n",
        "best_acc = 0\n",
        "best_config = None\n",
        "for ep, hl, opt, lr, wd in hyperparams:\n",
        "    train_loader = DataLoader(train_sub, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_sub, batch_size=64, shuffle=False)\n",
        "    model = FeedforwardNN(hidden_layers=hl).to(DEVICE)\n",
        "    optimizer = get_optimizer(model, opt, lr, wd)\n",
        "    acc = train(model, train_loader, val_loader, optimizer, nn.CrossEntropyLoss(), ep)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_config = (ep, hl, opt, lr, wd)\n",
        "    print(f\"Config: {ep} epochs, {hl} layers, {opt} optimizer, lr={lr}, wd={wd} => Accuracy: {acc}%\")\n",
        "\n",
        "print(f\"Best Config: {best_config} with Accuracy: {best_acc}%\")"
      ]
    }
  ],
 
}
